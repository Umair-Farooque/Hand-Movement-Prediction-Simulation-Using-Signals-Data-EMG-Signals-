# Handâ€‘Movement Prediction Using NinaPro Dataset

A complete pipeline for **EMGâ€‘based handâ€‘movement recognition** (24 gestures) **and realâ€‘time 3â€‘D visualisation in Blender**.

---

## Dataset

We use **NinaPro DBâ€‘1** and **DBâ€‘5**&nbsp;â‡¢Â <https://ninapro.hevs.ch/>.

> NinaPro defines 52 gestures; **this repo currently targets 24** (listed below).

### Movementâ€‘label map

| Label | Movement | Label | Movement |
|:----:|-------------------------------|:----:|----------------------------------------------|
| 0  | Rest                                          | 12 | Thumb Extension |
| 1  | Index Flexion                                | 13 | ThumbsÂ Up |
| 2  | Index Extension                              | 14 | Extension of indexÂ +Â middle; flex others |
| 3  | Middle Flexion                               | 15 | Flex ringÂ +Â pinky; extend others |
| 4  | Middle Extension                             | 16 | Thumb opposing base of little finger |
| 5  | Ring Flexion                                 | 17 | Abduction of all fingers |
| 6  | Ring Extension                               | 18 | Fist (all fingers flexed together) |
| 7  | Pinky Flexion                                | 19 | Pointing Index |
| 8  | Pinky Extension                              | 20 | Wrist Flexion |
| 9  | Thumb Adduction                              | 21 | Wrist Extension |
| 10 | Thumb Abduction                              | 22 | Wrist Extension with closed hand |
| 11 | Thumb Flexion                                | 23 | Ring Grasp |

![NinaPro gestures](https://ninapro.hevs.ch/figures/SData_Movements.png)

---

## Techniques

### Signal ProcessingÂ (`scipy`)
* **Bandâ€‘pass filter** â€“Â `signal.butter`Â + `signal.filtfilt`
* **Notch filter** â€“Â `signal.iirnotch` (50/60Â Hz mains)

### Machine LearningÂ (PyTorchÂ +Â sklearn)
* CNN / LSTM hybrids
* **Timeâ€‘series crossâ€‘validation** â€“Â `TimeSeriesSplit`
* **Metrics** â€“Â accuracy, macroâ€‘F1

### Data Handling
* **SciPy I/O** for `.mat`
* **NumPy** vectorisation
* **TQDM** progress bars

---

## ğŸ”§ SimulationÂ PipelineÂ (BlenderÂ 3D)

| # | Script | Purpose |
|:-:|--------|---------|
| 1 | `pre_saved_model.py` | Load pretrained weights |
| 2 | `request.py` | Stream/record EMG â†’ predict label â†’ write `prediction.json` |
| 3 | `blender/simulation.py` | Read JSON â†’ play matching **Action** on rigged hand |

<details>
<summary><strong>Why Blender (instead of Unity)?</strong></summary>

* Fully scriptable via PythonÂ API (`bpy`)
* Easy headless rendering/CI integration
* Lightweight for a singleâ€‘hand scene
</details>

### Folder structure

```text
project/
â”œâ”€â”€ data/                     # rawÂ & processed EMG
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model.pt
â”œâ”€â”€ blender/
â”‚   â”œâ”€â”€ HandRig.blend         # rigged hand with 24 actions
â”‚   â””â”€â”€ simulation.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ pre_saved_model.py
â”‚   â””â”€â”€ request.py
â””â”€â”€ prediction.json           # generated each inference cycle
```

### Quickâ€‘start

```bash
#Â 1. Predict from incoming EMG and save result
python scripts/request.py --model models/best_model.pt --out prediction.json

#Â 2. Animate inside Blender
blender blender/HandRig.blend --background        --python blender/simulation.py -- --json prediction.json --render
```

> Add `--render` to export an MP4; omit it for live viewport playback.

---

## Known IssuesÂ &Â Troubleshooting

| Symptom | Likely Cause / Fix |
|---------|--------------------|
| **âŒ  `Armature 'HandRig' not found`** | The armature in `HandRig.blend` must be named **exactly** `HandRig`. |
| **âŒ  `Action '<name>' not found`** | Ensure each of the 24 Blender Actions matches the movement names **verbatim** (see table above). |
| **Blender canâ€™t import `bpy`** | Run the script *inside* Blender (`blender â€¦ --python â€¦`). |
| **PythonÂ module errors** | Install requirements: `pip install -r requirements.txt` (SciPy, NumPy, PyTorch, scikitâ€‘learn, tqdm). |
| **Lag during live streaming** | Use a lower EMG sampling rate or batch predictions. |
| **Render is blank/black** | Add a Camera & light, or switch viewport render engine to Eevee/Cycles. |

---

## Contributing

1. **Fork** the repo  
2. **Create** a feature branch  
3. **Commit** clear, atomic changes  
4. **Open** a PR â€“Â please describe *what* & *why*  

We welcome PRs for:
* Additional NinaPro gestures (25â€“52)
* Realâ€‘time OSC / WebSocket streaming
* Rig/animation improvements
* Performance profiling & quantisation

---

*Happy coding & animating!Â ğŸ‰*
